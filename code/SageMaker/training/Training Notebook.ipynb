{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae749560-82c4-4677-a842-14a7404d7a71",
   "metadata": {},
   "source": [
    "# 1 - Acquire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c36073-6aa6-43b7-9224-5fc7f2f53f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-27 02:41:07--  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/f45bkkt8pr-1.zip\n",
      "Resolving md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)... 52.218.108.72\n",
      "Connecting to md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)|52.218.108.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 474313 (463K) [application/zip]\n",
      "Saving to: ‘f45bkkt8pr-1.zip’\n",
      "\n",
      "f45bkkt8pr-1.zip    100%[===================>] 463.20K  1.74MB/s    in 0.3s    \n",
      "\n",
      "2022-09-27 02:41:07 (1.74 MB/s) - ‘f45bkkt8pr-1.zip’ saved [474313/474313]\n",
      "\n",
      "Archive:  f45bkkt8pr-1.zip\n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Phone_extract.py  \n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Http_extract.py  \n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Email_extract.py  \n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/data_conversion.pdf  \n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Frequency.pdf  \n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/data_visual.pdf  \n",
      "  inflating: SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Dataset_5971.zip  \n",
      "Archive:  SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Dataset_5971.zip\n",
      "  inflating: Dataset_5971.csv        \n"
     ]
    }
   ],
   "source": [
    "!wget https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/f45bkkt8pr-1.zip\n",
    "!unzip f45bkkt8pr-1.zip\n",
    "!unzip \"SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION/Dataset_5971.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073a09f-48d2-470b-9380-35dd99996c9e",
   "metadata": {},
   "source": [
    "# 2 - Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5979d5-49eb-43fe-9b61-004ba75704ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.7/site-packages (2.109.0)\n",
      "Requirement already satisfied: transformers==4.12.3 in /opt/conda/lib/python3.7/site-packages (4.12.3)\n",
      "Requirement already satisfied: datasets[s3]==1.18.3 in /opt/conda/lib/python3.7/site-packages (1.18.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.12.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (2.28.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (0.0.53)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (0.9.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (0.10.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (4.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (2022.8.17)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (21.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (9.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (2022.7.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (0.70.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (1.3.5)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (1.27.62)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (1.24.62)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (0.4.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.20.1)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (21.4.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->datasets[s3]==1.18.3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->datasets[s3]==1.18.3) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore->datasets[s3]==1.18.3) (1.26.12)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore->datasets[s3]==1.18.3) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.3) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.3) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.48.0) (1.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.3) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.3) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.3) (2.0.4)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets[s3]==1.18.3) (2019.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (1.7.6.5)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.3) (0.14.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.3) (7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" \"torch\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b545c6a-367b-407b-b754-976b63413c82",
   "metadata": {},
   "source": [
    "# 3 - Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef85b93-0adc-4c17-bcd4-b22f242ebf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05cca97e-e4b1-491e-a40a-700796ca3510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::339199775262:role/service-role/AmazonSageMaker-ExecutionRole-20220926T232139\n",
      "sagemaker bucket: sagemaker-us-east-1-339199775262\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c58eb-eef0-4b9f-a30e-5929ff3c22df",
   "metadata": {},
   "source": [
    "# 4 - Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fb44ab-7640-4349-849b-1e8fa4ba282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9203787e-6b8d-4e24-b90d-a03524395f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1120f8a049d34989823caddfcfb1c926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6373ee5daf4e8b955383c3f1806b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d405bc5c54af436d8d252bae50de895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043cd5e524554e5d8d2f5cf4f9a08087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f23d9e9fcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d7da98e21e46e5910ac70d3868b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e613f3da2b430f9dfd75d3a77ea60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset_5971.csv')\n",
    "\n",
    "df.LABEL = (df.LABEL.str.lower()=='smishing')*1 #Binarizing problem\n",
    "df = df[['TEXT', 'LABEL']] #Filtering out extra columns\n",
    "df.columns = ['text', 'label_ids'] #Adjusting column names\n",
    "\n",
    "#Train/test split\n",
    "df_train, df_test = train_test_split(df, \n",
    "                                     test_size=0.2, \n",
    "                                     stratify=df.label_ids, \n",
    "                                     random_state=0)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train) #Intantiate train dataset from pandas\n",
    "test_dataset = Dataset.from_pandas(df_test) #Intantiate test dataset from pandas\n",
    "\n",
    "dataset = DatasetDict({'train':train_dataset, 'eval':test_dataset}) #Define join dataset\n",
    "\n",
    "#Download tokenizer\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "#Tokenizing function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], \n",
    "                     truncation=True, \n",
    "                     max_length=32, \n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "dataset = dataset.map(tokenize) #Apply tokenization\n",
    "\n",
    "dataset.set_format('torch', columns=['input_ids', \n",
    "                                     'attention_mask', \n",
    "                                     'special_tokens_mask', \n",
    "                                     'label_ids']) #Set format to torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797ef81-8583-4dd2-a0c4-be9a8c6e1745",
   "metadata": {},
   "source": [
    "# 5 - Upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83cf7dbd-2ca4-47b6-b95d-a20b0c737ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set saved to: s3://sagemaker-us-east-1-339199775262/data/train\n",
      "Eval set saved to: s3://sagemaker-us-east-1-339199775262/data/test\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "s3_prefix = \"data\"\n",
    "\n",
    "#Save train dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "dataset[\"train\"].save_to_disk(training_input_path, fs=s3)\n",
    "print(f\"Training set saved to: {training_input_path}\")\n",
    "\n",
    "#Save test dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "dataset[\"eval\"].save_to_disk(test_input_path, fs=s3)\n",
    "print(f\"Eval set saved to: {test_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae0bf67-8f46-4de8-8939-ee9f5850655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "#Training job hyper-parameters\n",
    "hyperparameters={'epochs': 4,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aeabb95-406d-4e9e-a7f5-fbaba3aa9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type=\"ml.g4dn.xlarge\",\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.12',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a8de88-9103-45c1-be96-04c5e947d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-27 02:41:37 Starting - Starting the training job...\n",
      "2022-09-27 02:42:04 Starting - Preparing the instances for trainingProfilerReport-1664246497: InProgress\n",
      ".........\n",
      "2022-09-27 02:43:33 Downloading - Downloading input data\n",
      "2022-09-27 02:43:33 Training - Downloading the training image.............................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-27 02:48:17,656 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-27 02:48:17,687 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-27 02:48:17,694 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-27 02:48:18,213 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 4,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-09-27-02-41-36-867\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-339199775262/huggingface-pytorch-training-2022-09-27-02-41-36-867/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":4,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-339199775262/huggingface-pytorch-training-2022-09-27-02-41-36-867/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":4,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-09-27-02-41-36-867\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-339199775262/huggingface-pytorch-training-2022-09-27-02-41-36-867/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"4\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 4 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m2022-09-27 02:48:22,649 - __main__ - INFO -  loaded train_dataset length is: 4776\u001b[0m\n",
      "\u001b[34m2022-09-27 02:48:22,649 - __main__ - INFO -  loaded test_dataset length is: 1195\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 483/483 [00:00<00:00, 489kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/256M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 7.50M/256M [00:00<00:03, 78.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 15.7M/256M [00:00<00:03, 83.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 24.2M/256M [00:00<00:02, 85.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 32.4M/256M [00:00<00:02, 82.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 40.2M/256M [00:00<00:02, 78.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 48.3M/256M [00:00<00:02, 80.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 56.2M/256M [00:00<00:02, 81.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 63.9M/256M [00:00<00:02, 79.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 72.8M/256M [00:00<00:02, 83.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 81.8M/256M [00:01<00:02, 86.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 91.1M/256M [00:01<00:01, 90.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 100M/256M [00:01<00:01, 92.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 110M/256M [00:01<00:01, 93.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 119M/256M [00:01<00:01, 90.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 127M/256M [00:01<00:01, 88.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 136M/256M [00:01<00:01, 89.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 145M/256M [00:01<00:01, 90.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 153M/256M [00:01<00:01, 85.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 161M/256M [00:01<00:01, 85.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 170M/256M [00:02<00:01, 84.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 178M/256M [00:02<00:00, 82.7MB/s]\u001b[0m\n",
      "\n",
      "2022-09-27 02:48:26 Training - Training image download completed. Training in progress.\u001b[34mDownloading:  73%|███████▎  | 186M/256M [00:02<00:00, 83.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 194M/256M [00:02<00:00, 80.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 202M/256M [00:02<00:00, 82.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 210M/256M [00:02<00:00, 83.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 218M/256M [00:02<00:00, 77.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 226M/256M [00:02<00:00, 75.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 233M/256M [00:02<00:00, 74.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 240M/256M [00:03<00:00, 74.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 249M/256M [00:03<00:00, 79.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 256M/256M [00:03<00:00, 83.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.6kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 25.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 22.0MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 4776\n",
      "  Num Epochs = 4\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 4776\n",
      "  Num Epochs = 4\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 600\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 600\u001b[0m\n",
      "\u001b[34m0%|          | 0/600 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.337 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.467 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.468 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.468 algo-1:27 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.469 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.469 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.601 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.602 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.603 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.604 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:591] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.605 algo-1:27 INFO hook.py:593] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.606 algo-1:27 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-09-27 02:48:32.606 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34m0%|          | 1/600 [00:02<22:10,  2.22s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/600 [00:02<06:25,  1.55it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5/600 [00:02<03:34,  2.77it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 6/600 [00:02<02:53,  3.43it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 8/600 [00:02<02:01,  4.87it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/600 [00:03<01:37,  6.08it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/600 [00:03<01:21,  7.19it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/600 [00:03<01:12,  8.03it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 16/600 [00:03<01:07,  8.71it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 18/600 [00:03<01:03,  9.23it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/600 [00:04<01:00,  9.51it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 22/600 [00:04<00:58,  9.87it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 24/600 [00:04<00:57,  9.96it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 26/600 [00:04<00:56, 10.17it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 28/600 [00:04<00:56, 10.18it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 30/600 [00:04<00:55, 10.21it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 32/600 [00:05<00:54, 10.33it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 34/600 [00:05<00:54, 10.36it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 36/600 [00:05<00:54, 10.39it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 38/600 [00:05<00:53, 10.43it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 40/600 [00:05<00:53, 10.45it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 42/600 [00:06<00:53, 10.52it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 44/600 [00:06<00:53, 10.47it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 46/600 [00:06<00:53, 10.36it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 48/600 [00:06<00:53, 10.36it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 50/600 [00:06<00:52, 10.47it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 52/600 [00:07<00:52, 10.41it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 54/600 [00:07<00:52, 10.35it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 56/600 [00:07<00:52, 10.38it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 58/600 [00:07<00:52, 10.38it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 60/600 [00:07<00:51, 10.48it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 62/600 [00:08<00:52, 10.34it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 64/600 [00:08<00:51, 10.36it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 66/600 [00:08<00:51, 10.37it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 68/600 [00:08<00:51, 10.33it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 70/600 [00:08<00:50, 10.42it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 72/600 [00:09<00:50, 10.51it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 74/600 [00:09<00:50, 10.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 76/600 [00:09<00:50, 10.47it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 78/600 [00:09<00:50, 10.43it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 80/600 [00:09<00:49, 10.43it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 82/600 [00:09<00:49, 10.40it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 84/600 [00:10<00:49, 10.42it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 86/600 [00:10<00:48, 10.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 88/600 [00:10<00:48, 10.53it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 90/600 [00:10<00:48, 10.52it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 92/600 [00:10<00:48, 10.43it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 94/600 [00:11<00:48, 10.51it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 96/600 [00:11<00:48, 10.47it/s]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 98/600 [00:11<00:47, 10.54it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 100/600 [00:11<00:47, 10.62it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 102/600 [00:11<00:47, 10.58it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 104/600 [00:12<00:46, 10.57it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 106/600 [00:12<00:46, 10.56it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 108/600 [00:12<00:46, 10.55it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 110/600 [00:12<00:46, 10.54it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 112/600 [00:12<00:46, 10.56it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 114/600 [00:13<00:46, 10.49it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 116/600 [00:13<00:45, 10.53it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 118/600 [00:13<00:45, 10.58it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 120/600 [00:13<00:45, 10.51it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 122/600 [00:13<00:45, 10.46it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 124/600 [00:13<00:45, 10.40it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 126/600 [00:14<00:45, 10.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 128/600 [00:14<00:45, 10.41it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 130/600 [00:14<00:45, 10.38it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 132/600 [00:14<00:44, 10.42it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 134/600 [00:14<00:44, 10.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 136/600 [00:15<00:44, 10.47it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 138/600 [00:15<00:43, 10.51it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 140/600 [00:15<00:44, 10.43it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 142/600 [00:15<00:43, 10.43it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 144/600 [00:15<00:43, 10.44it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 146/600 [00:16<00:43, 10.40it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 148/600 [00:16<00:42, 10.54it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 150/600 [00:16<00:40, 11.01it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 1195\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 1195\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/19 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 3/19 [00:00<00:00, 28.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 6/19 [00:00<00:00, 21.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 9/19 [00:00<00:00, 20.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 12/19 [00:00<00:00, 19.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 15/19 [00:00<00:00, 19.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 17/19 [00:00<00:00, 18.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10273825377225876, 'eval_accuracy': 0.9598326359832636, 'eval_precision': 0.773972602739726, 'eval_recall': 0.8828125, 'eval_f1': 0.8248175182481752, 'eval_runtime': 1.0088, 'eval_samples_per_second': 1184.524, 'eval_steps_per_second': 18.833, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 150/600 [00:17<00:40, 11.01it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 19/19 [00:00<00:00, 18.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 152/600 [00:17<01:48,  4.12it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 154/600 [00:17<01:28,  5.02it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 156/600 [00:18<01:14,  5.96it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 158/600 [00:18<01:04,  6.85it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 160/600 [00:18<00:57,  7.60it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 162/600 [00:18<00:52,  8.29it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 164/600 [00:18<00:49,  8.79it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 166/600 [00:18<00:46,  9.25it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 168/600 [00:19<00:45,  9.57it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 170/600 [00:19<00:43,  9.80it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 172/600 [00:19<00:42,  9.96it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 174/600 [00:19<00:42, 10.11it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 176/600 [00:19<00:41, 10.21it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 178/600 [00:20<00:41, 10.24it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 180/600 [00:20<00:40, 10.42it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 182/600 [00:20<00:40, 10.35it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 184/600 [00:20<00:40, 10.35it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 186/600 [00:20<00:39, 10.43it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 188/600 [00:21<00:39, 10.32it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 190/600 [00:21<00:39, 10.47it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 192/600 [00:21<00:38, 10.51it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 194/600 [00:21<00:38, 10.42it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 196/600 [00:21<00:38, 10.48it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 198/600 [00:22<00:38, 10.54it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 200/600 [00:22<00:37, 10.54it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 202/600 [00:22<00:37, 10.55it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 204/600 [00:22<00:38, 10.41it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 206/600 [00:22<00:37, 10.50it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 208/600 [00:22<00:37, 10.47it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 210/600 [00:23<00:37, 10.46it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 212/600 [00:23<00:36, 10.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 214/600 [00:23<00:36, 10.50it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 216/600 [00:23<00:36, 10.53it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 218/600 [00:23<00:36, 10.41it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 220/600 [00:24<00:36, 10.45it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 222/600 [00:24<00:36, 10.46it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 224/600 [00:24<00:36, 10.38it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 226/600 [00:24<00:35, 10.43it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 228/600 [00:24<00:35, 10.43it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 230/600 [00:25<00:35, 10.42it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 232/600 [00:25<00:35, 10.40it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 234/600 [00:25<00:35, 10.33it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 236/600 [00:25<00:35, 10.35it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 238/600 [00:25<00:34, 10.43it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 240/600 [00:26<00:34, 10.47it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 242/600 [00:26<00:34, 10.41it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 244/600 [00:26<00:34, 10.43it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 246/600 [00:26<00:33, 10.45it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 248/600 [00:26<00:33, 10.47it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 250/600 [00:27<00:33, 10.39it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 252/600 [00:27<00:33, 10.50it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 254/600 [00:27<00:32, 10.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 256/600 [00:27<00:33, 10.37it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 258/600 [00:27<00:32, 10.48it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 260/600 [00:27<00:32, 10.41it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 262/600 [00:28<00:32, 10.33it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 264/600 [00:28<00:32, 10.41it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 266/600 [00:28<00:32, 10.34it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 268/600 [00:28<00:31, 10.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 270/600 [00:28<00:31, 10.42it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 272/600 [00:29<00:31, 10.43it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 274/600 [00:29<00:30, 10.57it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 276/600 [00:29<00:30, 10.56it/s]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 278/600 [00:29<00:30, 10.39it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 280/600 [00:29<00:30, 10.35it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 282/600 [00:30<00:30, 10.27it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 284/600 [00:30<00:30, 10.40it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 286/600 [00:30<00:30, 10.44it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 288/600 [00:30<00:30, 10.39it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 290/600 [00:30<00:29, 10.36it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 292/600 [00:31<00:29, 10.45it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 294/600 [00:31<00:29, 10.46it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 296/600 [00:31<00:29, 10.44it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 298/600 [00:31<00:29, 10.39it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 300/600 [00:31<00:27, 11.01it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 1195\u001b[0m\n",
      "\u001b[34mNum examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mBatch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/19 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 3/19 [00:00<00:00, 26.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 6/19 [00:00<00:00, 20.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 9/19 [00:00<00:00, 19.37it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 11/19 [00:00<00:00, 18.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 13/19 [00:00<00:00, 18.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 15/19 [00:00<00:00, 18.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 17/19 [00:00<00:00, 18.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10009311884641647, 'eval_accuracy': 0.9656903765690377, 'eval_precision': 0.8270676691729323, 'eval_recall': 0.859375, 'eval_f1': 0.8429118773946361, 'eval_runtime': 1.0398, 'eval_samples_per_second': 1149.226, 'eval_steps_per_second': 18.272, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 300/600 [00:32<00:27, 11.01it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 19/19 [00:00<00:00, 18.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 302/600 [00:33<01:14,  4.02it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 304/600 [00:33<01:00,  4.92it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 306/600 [00:33<00:50,  5.81it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 308/600 [00:33<00:43,  6.71it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 310/600 [00:33<00:38,  7.52it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 312/600 [00:33<00:35,  8.22it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 314/600 [00:34<00:32,  8.82it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 316/600 [00:34<00:30,  9.19it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 318/600 [00:34<00:29,  9.52it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 320/600 [00:34<00:28,  9.78it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 322/600 [00:34<00:28,  9.90it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 324/600 [00:35<00:27, 10.09it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 326/600 [00:35<00:27, 10.13it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 328/600 [00:35<00:26, 10.29it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 330/600 [00:35<00:26, 10.28it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 332/600 [00:35<00:26, 10.30it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 334/600 [00:36<00:26, 10.19it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 336/600 [00:36<00:25, 10.27it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 338/600 [00:36<00:25, 10.39it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 340/600 [00:36<00:24, 10.45it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 342/600 [00:36<00:24, 10.44it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 344/600 [00:37<00:24, 10.41it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 346/600 [00:37<00:24, 10.44it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 348/600 [00:37<00:24, 10.42it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 350/600 [00:37<00:24, 10.41it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 352/600 [00:37<00:23, 10.46it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 354/600 [00:38<00:23, 10.40it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 356/600 [00:38<00:23, 10.43it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 358/600 [00:38<00:23, 10.30it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 360/600 [00:38<00:23, 10.36it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 362/600 [00:38<00:23, 10.31it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 364/600 [00:38<00:22, 10.43it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 366/600 [00:39<00:22, 10.44it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 368/600 [00:39<00:22, 10.35it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 370/600 [00:39<00:22, 10.43it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 372/600 [00:39<00:22, 10.33it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 374/600 [00:39<00:21, 10.40it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 376/600 [00:40<00:21, 10.37it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 378/600 [00:40<00:21, 10.39it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 380/600 [00:40<00:21, 10.42it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 382/600 [00:40<00:20, 10.40it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 384/600 [00:40<00:20, 10.45it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 386/600 [00:41<00:20, 10.46it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 388/600 [00:41<00:20, 10.50it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 390/600 [00:41<00:20, 10.48it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 392/600 [00:41<00:19, 10.42it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 394/600 [00:41<00:19, 10.46it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 396/600 [00:42<00:19, 10.45it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 398/600 [00:42<00:19, 10.41it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 400/600 [00:42<00:19, 10.42it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 402/600 [00:42<00:19, 10.29it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 404/600 [00:42<00:19, 10.27it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 406/600 [00:43<00:18, 10.23it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 408/600 [00:43<00:18, 10.31it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 410/600 [00:43<00:18, 10.32it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 412/600 [00:43<00:18, 10.35it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 414/600 [00:43<00:17, 10.40it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 416/600 [00:43<00:17, 10.43it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 418/600 [00:44<00:17, 10.40it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 420/600 [00:44<00:17, 10.30it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 422/600 [00:44<00:17, 10.33it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 424/600 [00:44<00:17, 10.33it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 426/600 [00:44<00:16, 10.41it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 428/600 [00:45<00:16, 10.38it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 430/600 [00:45<00:16, 10.35it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 432/600 [00:45<00:16, 10.29it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 434/600 [00:45<00:16, 10.26it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 436/600 [00:45<00:15, 10.33it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 438/600 [00:46<00:15, 10.36it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 440/600 [00:46<00:15, 10.34it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 442/600 [00:46<00:15, 10.35it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 444/600 [00:46<00:15, 10.36it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 446/600 [00:46<00:14, 10.44it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 448/600 [00:47<00:14, 10.36it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 450/600 [00:47<00:13, 11.03it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/19 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 3/19 [00:00<00:00, 26.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 6/19 [00:00<00:00, 20.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 9/19 [00:00<00:00, 19.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 12/19 [00:00<00:00, 18.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 14/19 [00:00<00:00, 18.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 16/19 [00:00<00:00, 18.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 18/19 [00:00<00:00, 18.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.11193191260099411, 'eval_accuracy': 0.9673640167364017, 'eval_precision': 0.856, 'eval_recall': 0.8359375, 'eval_f1': 0.8458498023715415, 'eval_runtime': 1.0404, 'eval_samples_per_second': 1148.597, 'eval_steps_per_second': 18.262, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 450/600 [00:48<00:13, 11.03it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 19/19 [00:00<00:00, 18.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 452/600 [00:48<00:36,  4.03it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 454/600 [00:48<00:29,  4.95it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 456/600 [00:48<00:24,  5.87it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 458/600 [00:49<00:21,  6.74it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 459/600 [00:49<00:19,  7.14it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 461/600 [00:49<00:17,  7.93it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 463/600 [00:49<00:15,  8.57it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 465/600 [00:49<00:14,  9.00it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 467/600 [00:49<00:14,  9.31it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 469/600 [00:50<00:13,  9.66it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 471/600 [00:50<00:13,  9.89it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 473/600 [00:50<00:12, 10.09it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 475/600 [00:50<00:12, 10.08it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 477/600 [00:50<00:12, 10.21it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 479/600 [00:51<00:11, 10.18it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 481/600 [00:51<00:11, 10.23it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 483/600 [00:51<00:11, 10.28it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 485/600 [00:51<00:11, 10.29it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 487/600 [00:51<00:10, 10.31it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 489/600 [00:52<00:10, 10.25it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 491/600 [00:52<00:10, 10.25it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 493/600 [00:52<00:10, 10.35it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 495/600 [00:52<00:10, 10.39it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 497/600 [00:52<00:09, 10.38it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 499/600 [00:53<00:09, 10.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1392, 'learning_rate': 5e-05, 'epoch': 3.33}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 500/600 [00:53<00:09, 10.45it/s]\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model/checkpoint-500\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-500/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-500/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/checkpoint-500/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/checkpoint-500/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 501/600 [00:54<00:32,  3.01it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 502/600 [00:54<00:28,  3.43it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 504/600 [00:55<00:21,  4.42it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 505/600 [00:55<00:19,  4.95it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 507/600 [00:55<00:15,  6.06it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 508/600 [00:55<00:13,  6.59it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 510/600 [00:55<00:11,  7.58it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 512/600 [00:55<00:10,  8.37it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 514/600 [00:56<00:09,  8.95it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 516/600 [00:56<00:08,  9.34it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 518/600 [00:56<00:08,  9.63it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 520/600 [00:56<00:08,  9.80it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 522/600 [00:56<00:07,  9.99it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 524/600 [00:57<00:07, 10.07it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 526/600 [00:57<00:07, 10.15it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 528/600 [00:57<00:07, 10.21it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 530/600 [00:57<00:06, 10.30it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 532/600 [00:57<00:06, 10.23it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 534/600 [00:57<00:06, 10.25it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 536/600 [00:58<00:06, 10.35it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 538/600 [00:58<00:06, 10.31it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 540/600 [00:58<00:05, 10.37it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 542/600 [00:58<00:05, 10.25it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 544/600 [00:58<00:05, 10.30it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 546/600 [00:59<00:05, 10.29it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 548/600 [00:59<00:04, 10.42it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 550/600 [00:59<00:04, 10.43it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 552/600 [00:59<00:04, 10.32it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 554/600 [00:59<00:04, 10.38it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 556/600 [01:00<00:04, 10.35it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 558/600 [01:00<00:04, 10.36it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 560/600 [01:00<00:03, 10.38it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 562/600 [01:00<00:03, 10.41it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 564/600 [01:00<00:03, 10.40it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 566/600 [01:01<00:03, 10.39it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 568/600 [01:01<00:03, 10.37it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 570/600 [01:01<00:02, 10.35it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 572/600 [01:01<00:02, 10.31it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 574/600 [01:01<00:02, 10.36it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 576/600 [01:02<00:02, 10.34it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 578/600 [01:02<00:02, 10.35it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 580/600 [01:02<00:01, 10.35it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 582/600 [01:02<00:01, 10.37it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 584/600 [01:02<00:01, 10.39it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 586/600 [01:02<00:01, 10.37it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 588/600 [01:03<00:01, 10.38it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 590/600 [01:03<00:00, 10.41it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 592/600 [01:03<00:00, 10.43it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 594/600 [01:03<00:00, 10.44it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 596/600 [01:03<00:00, 10.35it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 598/600 [01:04<00:00, 10.38it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 600/600 [01:04<00:00, 11.18it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\n",
      "  Num examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/19 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 3/19 [00:00<00:00, 27.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 6/19 [00:00<00:00, 20.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 9/19 [00:00<00:00, 19.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 12/19 [00:00<00:00, 18.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 14/19 [00:00<00:00, 18.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 16/19 [00:00<00:00, 18.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 18/19 [00:00<00:00, 18.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.10500170290470123, 'eval_accuracy': 0.9615062761506277, 'eval_precision': 0.7928571428571428, 'eval_recall': 0.8671875, 'eval_f1': 0.8283582089552239, 'eval_runtime': 1.0417, 'eval_samples_per_second': 1147.138, 'eval_steps_per_second': 18.239, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 600/600 [01:05<00:00, 11.18it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 19/19 [00:00<00:00, 18.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m100%|██████████| 600/600 [01:05<00:00, 11.18it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 65.3447, 'train_samples_per_second': 292.357, 'train_steps_per_second': 9.182, 'train_loss': 0.12356456995010376, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 600/600 [01:05<00:00,  9.18it/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34mThe following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: special_tokens_mask, __index_level_0__, text.\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34mNum examples = 1195\n",
      "  Batch size = 64\u001b[0m\n",
      "\u001b[34m0%|          | 0/19 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 3/19 [00:00<00:00, 25.75it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 6/19 [00:00<00:00, 20.72it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 9/19 [00:00<00:00, 19.28it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 11/19 [00:00<00:00, 18.84it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 13/19 [00:00<00:00, 18.71it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 15/19 [00:00<00:00, 18.58it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 17/19 [00:00<00:00, 18.49it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 19/19 [00:00<00:00, 19.22it/s]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2022-09-27 02:49:39,749 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-09-27 02:49:39,749 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-09-27 02:49:39,750 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-09-27 02:50:06 Uploading - Uploading generated training model\n",
      "2022-09-27 02:52:06 Completed - Training job completed\n",
      "Training seconds: 529\n",
      "Billable seconds: 529\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25320a42-ce6a-41ec-a838-578dec970632",
   "metadata": {},
   "source": [
    "### Deploying endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219dffb1-2995-4834-a463-30b1db6d1ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8824ee82-6d3f-4587-855c-066d366b8790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9754716157913208}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input= {\"inputs\":'Dear PAYTM customer your Paytm KYC has expired. Contact customer care No-8536074310 immediately. your account will Block within 24 hr. Thank you PAYTM TEAM.'}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "344ad5d2-dcda-4feb-b0c0-3a5e14479a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-training-2022-09-27-02-52-47-564'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
